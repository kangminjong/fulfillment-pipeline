"""
âœ… Kafka 'event' í† í”½ì—ì„œ ì£¼ë¬¸ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ì—¬ Postgresì— ì ì¬ (ë°°ì¹˜/íŠ¸ëœì­ì…˜ ê¸°ë°˜)

[í•µì‹¬ ì •ì • ì‚¬í•­ - ìŠ¤í‚¤ë§ˆ ê¸°ì¤€]
- orders_raw = "ì´ìƒ ë°ì´í„° ì „ìš©"ì´ ì•„ë‹ˆë¼, âœ… ëª¨ë“  ì›ë³¸(raw_payload)ì„ í•­ìƒ ì €ì¥í•˜ëŠ” í…Œì´ë¸”
- orders      = ì •ìƒ ë°ì´í„°(í•„ìˆ˜ê°’ ì¶©ì¡±)ë§Œ ìŠ¤ëƒ…ìƒ· upsert, âœ… raw_reference_id ë°˜ë“œì‹œ í¬í•¨(orders_raw.raw_id FK)
- events      = ê°€ëŠ¥í•œ í•œ í•­ìƒ ì €ì¥(ì›ì¥)
- ì´ìƒ ë°ì´í„°(user_id or shipping_address ëˆ„ë½ ë“±)ëŠ” âœ… ë³„ë„ í…Œì´ë¸”(ì˜ˆ: orders_invalid)ì— ì €ì¥

[ì²˜ë¦¬ ìˆœì„œ(íŠ¸ëœì­ì…˜ 1ë²ˆ)]
1) orders_raw  : ë°°ì¹˜ ì „ì²´ ì›ë³¸ì„ ë¨¼ì € ë²Œí¬ insert + RETURNING raw_id ë¡œ raw_id í™•ë³´
2) events      : í•­ìƒ insert (ON CONFLICT DO NOTHING)
3) orders      : ì •ìƒ ë°ì´í„°ë§Œ upsert (raw_reference_id=raw_id í¬í•¨)
4) orders_invalid: ì´ìƒ ë°ì´í„° ê¸°ë¡ (raw_reference_id=raw_id í¬í•¨)
5) DB commit ì„±ê³µ í›„ì—ë§Œ Kafka offset commit

â€» ì°¸ê³ : orders_invalid í…Œì´ë¸”ì´ DBì— ì—†ìœ¼ë©´, ì•„ë˜ DDLë¡œ ìƒì„± í•„ìš”
-----------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS public.orders_invalid (
  invalid_id        BIGSERIAL PRIMARY KEY,
  raw_reference_id  BIGINT NOT NULL REFERENCES public.orders_raw(raw_id),
  event_id          TEXT NULL,
  order_id          TEXT NULL,
  missing_fields    TEXT[] NOT NULL,
  detected_at       TIMESTAMPTZ DEFAULT now(),
  note              TEXT NULL
);
-----------------------------------------------------------------------
"""

import json
import os
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Optional

import psycopg2
from psycopg2.extras import Json, execute_values
from kafka import KafkaConsumer
from kafka.structs import TopicPartition, OffsetAndMetadata


# =============================================================================
# í™˜ê²½ë³€ìˆ˜ (docker-compose ê¸°ì¤€ ê¶Œì¥)
# =============================================================================
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "event")
KAFKA_GROUP_ID = os.getenv("KAFKA_GROUP_ID", "order-reader")
AUTO_OFFSET_RESET = os.getenv("AUTO_OFFSET_RESET", "earliest")

# âœ… ë°°ì¹˜ ì„¤ì •
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "1000"))              # ìµœëŒ€ 1000ê°œì”©
FLUSH_EVERY_SEC = float(os.getenv("FLUSH_EVERY_SEC", "1.0"))   # 1ì´ˆë§ˆë‹¤ flush (ëœ ì°¨ë„)

# âœ… íŒ€ DB ì ‘ì† ê·œì¹™: localhost ì‚¬ìš© ì•ˆ í•¨ (ê¸°ë³¸ê°’ì€ íŒ€ DB IPë¡œ)
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "192.168.239.40")
POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
POSTGRES_DB = os.getenv("POSTGRES_DB", "fulfillment")
POSTGRES_USER = os.getenv("POSTGRES_USER", "admin")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "admin")


# =============================================================================
# ìœ í‹¸: ì‹œê°„/íƒ€ì… íŒŒì‹±
# =============================================================================
def now_utc() -> datetime:
    """í˜„ì¬ UTC ì‹œê°„(aware datetime)"""
    return datetime.now(timezone.utc)


def parse_iso_datetime(value) -> datetime:
    """
    - None/ë¹ˆê°’ì´ë©´ í˜„ì¬ UTC
    - strì´ë©´ ISO8601 íŒŒì‹± (Z -> +00:00 ì²˜ë¦¬)
    - datetimeì´ë©´ tz ì—†ëŠ” ê²½ìš° UTCë¡œ ê°„ì£¼
    """
    if not value:
        return now_utc()

    if isinstance(value, datetime):
        return value if value.tzinfo else value.replace(tzinfo=timezone.utc)

    if isinstance(value, str):
        v = value.strip()
        try:
            if v.endswith("Z"):
                v = v[:-1] + "+00:00"
            dt = datetime.fromisoformat(v)
            return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
        except Exception:
            return now_utc()

    return now_utc()


def to_text_or_json(value):
    """
    shipping_address(text)ì— dict/listê°€ ë“¤ì–´ì˜¤ë©´ ì˜¤ë¥˜ ê°€ëŠ¥
    - dict/list -> JSON ë¬¸ìì—´
    - ê¸°íƒ€ -> str
    """
    if value is None:
        return None
    if isinstance(value, (dict, list)):
        return json.dumps(value, ensure_ascii=False)
    return str(value)


def stable_event_id(
    order_id: Optional[str],
    event_type: str,
    occurred_at: datetime,
    *,
    topic: Optional[str] = None,
    partition: Optional[int] = None,
    offset: Optional[int] = None,
) -> str:
    """
    producerê°€ event_idë¥¼ ì•ˆ ì£¼ëŠ” ê²½ìš°ì—ë„ "ì¬ì²˜ë¦¬ ì‹œ" ì¤‘ë³µ insertê°€ ëœ ìƒê¸°ê²Œ
    ê²°ì •ì  UUID(uuid5) ìƒì„±

    - order_idê°€ ìˆìœ¼ë©´: (order_id | event_type | occurred_at) ê¸°ë°˜
    - order_idê°€ ì—†ìœ¼ë©´: (topic | partition | offset | event_type | occurred_at) ê¸°ë°˜
    """
    if order_id:
        base = f"{order_id}|{event_type}|{occurred_at.isoformat()}"
        return str(uuid.uuid5(uuid.NAMESPACE_DNS, base))

    base = f"{topic}|{partition}|{offset}|{event_type}|{occurred_at.isoformat()}"
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, base))


# =============================================================================
# DB ì—°ê²° (ì¬ì‹œë„)
# =============================================================================
def connect_db_with_retry():
    """DB ì—°ê²°ì´ ë  ë•Œê¹Œì§€ 3ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„"""
    while True:
        try:
            conn = psycopg2.connect(
                host=POSTGRES_HOST,
                port=POSTGRES_PORT,
                dbname=POSTGRES_DB,
                user=POSTGRES_USER,
                password=POSTGRES_PASSWORD,
            )
            conn.autocommit = False  # âœ… íŠ¸ëœì­ì…˜ ì§ì ‘ ì œì–´
            print("âœ… Postgres ì—°ê²° ì„±ê³µ")
            return conn
        except Exception as e:
            print(f"â³ Postgres ì—°ê²° ì‹¤íŒ¨: {e} (3ì´ˆ í›„ ì¬ì‹œë„)")
            time.sleep(3)


# =============================================================================
# SQL (í˜„ì¬ DB êµ¬ì¡° ê¸°ì¤€)
# =============================================================================

# 1) orders_raw: âœ… ëª¨ë“  ì›ë³¸ ì €ì¥ + raw_id ë°˜í™˜(orders/orders_invalidê°€ ì°¸ì¡°í•´ì•¼ í•¨)
SQL_INSERT_ORDERS_RAW_VALUES_RETURNING = """
INSERT INTO public.orders_raw (
  raw_payload,
  kafka_offset,
  ingested_at
) VALUES %s
RETURNING raw_id;
"""

# 2) events: ì´ë²¤íŠ¸ ì›ì¥ (ê°€ëŠ¥í•œ í•œ í•­ìƒ ì €ì¥)
SQL_INSERT_EVENTS_VALUES = """
INSERT INTO public.events (
  event_id,
  order_id,
  event_type,
  current_status,
  reason_code,
  occurred_at,
  ingested_at,
  ops_status,
  ops_note,
  ops_operator,
  ops_updated_at
) VALUES %s
ON CONFLICT (event_id) DO NOTHING;
"""

# 3) orders: ìŠ¤ëƒ…ìƒ· upsert (âœ… ì •ìƒ ë°ì´í„°ë§Œ + raw_reference_id í•„ìˆ˜)
SQL_UPSERT_ORDERS_VALUES = """
INSERT INTO public.orders (
  order_id,
  user_id,
  product_id,
  product_name,
  shipping_address,
  current_stage,
  current_status,
  last_event_type,
  last_occurred_at,
  hold_reason_code,
  hold_ops_status,
  hold_ops_note,
  hold_ops_operator,
  hold_ops_updated_at,
  raw_reference_id
) VALUES %s
ON CONFLICT (order_id)
DO UPDATE SET
  user_id = EXCLUDED.user_id,
  product_id = EXCLUDED.product_id,
  product_name = EXCLUDED.product_name,
  shipping_address = EXCLUDED.shipping_address,
  current_stage = EXCLUDED.current_stage,
  current_status = EXCLUDED.current_status,
  last_event_type = EXCLUDED.last_event_type,
  last_occurred_at = EXCLUDED.last_occurred_at,
  hold_reason_code = EXCLUDED.hold_reason_code,
  hold_ops_status = EXCLUDED.hold_ops_status,
  hold_ops_note = EXCLUDED.hold_ops_note,
  hold_ops_operator = EXCLUDED.hold_ops_operator,
  hold_ops_updated_at = EXCLUDED.hold_ops_updated_at,
  raw_reference_id = EXCLUDED.raw_reference_id;
"""

# 4) orders_invalid: ì´ìƒ ë°ì´í„° ê¸°ë¡ (âœ… í…Œì´ë¸”ì´ DBì— ìˆì–´ì•¼ í•¨)
SQL_INSERT_ORDERS_INVALID_VALUES = """
INSERT INTO public.orders_invalid (
  raw_reference_id,
  event_id,
  order_id,
  missing_fields,
  detected_at,
  note
) VALUES %s;
"""

# 5) ë³´ê°• ì¡°íšŒ: HOLD ë“±ì—ì„œ product_id/product_nameì´ ëˆ„ë½ë  ìˆ˜ ìˆì–´ ordersì—ì„œ ë³´ê°•
SQL_SELECT_FROM_ORDERS = """
SELECT user_id, product_id, product_name, shipping_address
FROM public.orders
WHERE order_id = %s
LIMIT 1;
"""


# =============================================================================
# ë°°ì¹˜ ë¡œìš° êµ¬ì„± í•¨ìˆ˜ (orders_rawëŠ” í•­ìƒ ì €ì¥í•˜ë¯€ë¡œ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ”)
# =============================================================================
def build_orders_raw_rows(messages) -> Tuple[List[Tuple], Dict[Tuple[str, int], int]]:
    """
    âœ… 1ë‹¨ê³„: orders_rawì— ë„£ì„ rowsë¥¼ ë§Œë“ ë‹¤ (í•­ìƒ ì €ì¥)
    - raw_payloadì—ëŠ” ì›ë³¸ eventì— _metaë¥¼ ë¶™ì—¬ë‘ë©´ ì¶”ì /ë””ë²„ê¹…ì´ í¸í•´ì§
    - offset ì»¤ë°‹ì„ ìœ„í•´ íŒŒí‹°ì…˜ë³„ ë§ˆì§€ë§‰ offsetë„ í•¨ê»˜ ê³„ì‚°
    """
    orders_raw_rows: List[Tuple] = []
    last_offsets: Dict[Tuple[str, int], int] = {}  # {(topic, partition): last_offset}

    for msg in messages:
        # Kafka value_deserializerë¡œ ì´ë¯¸ dictê°€ ë“¤ì–´ì˜¤ëŠ” ê±¸ ê¸°ëŒ€í•˜ì§€ë§Œ ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬
        event = msg.value if isinstance(msg.value, dict) else {}

        ingested_at = now_utc()

        payload_for_raw = dict(event)
        payload_for_raw["_meta"] = {
            "kafka_topic": msg.topic,
            "kafka_partition": msg.partition,
            "kafka_offset": msg.offset,
            "ingested_at": ingested_at.isoformat(),
        }

        # orders_raw: (raw_payload, kafka_offset, ingested_at)
        orders_raw_rows.append((Json(payload_for_raw), msg.offset, ingested_at))

        # offset ì»¤ë°‹ ê³„ì‚°ìš© (íŒŒí‹°ì…˜ë³„ ë§ˆì§€ë§‰ offset ê°±ì‹ )
        last_offsets[(msg.topic, msg.partition)] = msg.offset

    return orders_raw_rows, last_offsets


def build_events_orders_invalid_rows(
    cur,
    messages,
    raw_ids: List[int],
) -> Tuple[List[Tuple], List[Tuple], List[Tuple]]:
    """
    âœ… 2ë‹¨ê³„: orders_rawì—ì„œ RETURNING ë°›ì€ raw_ids(ë©”ì‹œì§€ì™€ 1:1)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    - events  : ê°€ëŠ¥í•œ í•œ í•­ìƒ ì €ì¥
    - orders  : ì •ìƒ ë°ì´í„°(user_id, shipping_address, order_id í•„ìˆ˜)ë§Œ upsert
    - invalid : ì´ìƒ ë°ì´í„°ëŠ” orders_invalidì— ê¸°ë¡

    â€» ê°™ì€ ë°°ì¹˜ì—ì„œ ë™ì¼ order_id ë³´ê°• ì¡°íšŒ ìµœì í™”ë¥¼ ìœ„í•´ ìºì‹œ ì‚¬ìš©
    """
    events_rows: List[Tuple] = []
    orders_rows: List[Tuple] = []
    invalid_rows: List[Tuple] = []

    # âœ… ë³´ê°• ì¡°íšŒ ìºì‹œ
    orders_cache: Dict[str, Optional[Tuple]] = {}

    # ë©”ì‹œì§€ì™€ raw_idëŠ” ë°˜ë“œì‹œ ê°™ì€ ê¸¸ì´/ìˆœì„œì—¬ì•¼ í•¨
    for msg, raw_id in zip(messages, raw_ids):
        event = msg.value if isinstance(msg.value, dict) else {}

        # ---------------------------
        # (A) í•„ë“œ ì¶”ì¶œ/ì •ê·œí™”
        # ---------------------------
        order_id = event.get("order_id")

        # producer: customer_id â†’ consumer/DB: user_id
        user_id = event.get("user_id") or event.get("customer_id")

        current_stage = event.get("current_stage")
        current_status = event.get("current_status")

        # event_type ìš°ì„ ìˆœìœ„:
        # - last_event_type(ìŠ¤ëƒ…ìƒ·ìš©) -> event_type -> current_status -> UNKNOWN
        event_type = (
            event.get("last_event_type")
            or event.get("event_type")
            or current_status
            or "UNKNOWN"
        )

        occurred_at = parse_iso_datetime(
            event.get("last_occurred_at") or event.get("occurred_at")
        )
        ingested_at = now_utc()

        product_id = event.get("product_id")
        product_name = event.get("product_name")

        shipping_address = to_text_or_json(
            event.get("shipping_address") or event.get("address")
        )

        # reason_code (events.reason_code / orders.hold_reason_code)
        reason_code = event.get("reason_code") or event.get("hold_reason_code")

        # events ops ì»¬ëŸ¼
        ops_status = event.get("ops_status")
        ops_note = event.get("ops_note") or event.get("ops_comment")
        ops_operator = event.get("ops_operator") or event.get("ops_user")
        ops_updated_at = (
            parse_iso_datetime(event.get("ops_updated_at"))
            if event.get("ops_updated_at")
            else None
        )

        # orders hold_ops ì»¬ëŸ¼
        hold_ops_status = event.get("hold_ops_status")
        hold_ops_note = event.get("hold_ops_note") or event.get("hold_ops_comment")
        hold_ops_operator = event.get("hold_ops_operator") or event.get("hold_ops_user")
        hold_ops_updated_at = (
            parse_iso_datetime(event.get("hold_ops_updated_at"))
            if event.get("hold_ops_updated_at")
            else None
        )

        # event_id: producer ì œê³µ ì‹œ ì‚¬ìš©, ì—†ìœ¼ë©´ ê²°ì •ì  ìƒì„±
        event_id = event.get("event_id") or stable_event_id(
            order_id,
            event_type,
            occurred_at,
            topic=msg.topic,
            partition=msg.partition,
            offset=msg.offset,
        )

        # ---------------------------
        # (B) eventsëŠ” ê°€ëŠ¥í•œ í•œ í•­ìƒ ì €ì¥
        # ---------------------------
        events_rows.append(
            (
                event_id,
                order_id,
                event_type or "UNKNOWN",
                current_status or event_type or "UNKNOWN",
                reason_code,
                occurred_at,
                ingested_at,
                ops_status,
                ops_note,
                ops_operator,
                ops_updated_at,
            )
        )

        # ---------------------------
        # (C) ì •ìƒ/ì´ìƒ íŒì •
        # ---------------------------
        missing_fields = []

        if not order_id:
            missing_fields.append("order_id")
        if not user_id:
            missing_fields.append("user_id")
        if not shipping_address:
            missing_fields.append("shipping_address")

        is_invalid = len(missing_fields) > 0

        # ---------------------------
        # (D) ì´ìƒ ë°ì´í„° â†’ orders_invalid ê¸°ë¡
        # ---------------------------
        if is_invalid:
            invalid_rows.append(
                (
                    raw_id,            # raw_reference_id (FK)
                    event_id,          # event_id (ì¶”ì ìš©)
                    order_id,          # order_id (ì—†ì„ ìˆ˜ë„)
                    missing_fields,    # TEXT[]ë¡œ ì €ì¥
                    ingested_at,       # detected_at
                    "Missing required fields for orders snapshot",  # note
                )
            )
            continue

        # ---------------------------
        # (E) ì •ìƒ ë°ì´í„° â†’ orders upsert
        #     (HOLD ë“±ì—ì„œ product_id/product_name ëˆ„ë½ ì‹œ ordersì—ì„œ ë³´ê°•)
        # ---------------------------
        if order_id and (not product_id or not product_name):
            if order_id in orders_cache:
                cached = orders_cache[order_id]
            else:
                cur.execute(SQL_SELECT_FROM_ORDERS, (order_id,))
                cached = cur.fetchone()
                orders_cache[order_id] = cached

            if cached:
                existing_user_id, existing_product_id, existing_product_name, existing_shipping_address = cached
                # ì•ˆì „ ë³´ê°•
                user_id = user_id or existing_user_id
                shipping_address = shipping_address or existing_shipping_address
                product_id = product_id or existing_product_id
                product_name = product_name or existing_product_name

        # ë³´ê°• í›„ì—ë„ í•„ìˆ˜ê°’ì´ ë¹„ë©´(ë“œë¬¼ì§€ë§Œ) invalidë¡œ ë„˜ê¸°ëŠ” ê²Œ ì•ˆì „
        missing_after_enrich = []
        if not order_id:
            missing_after_enrich.append("order_id")
        if not user_id:
            missing_after_enrich.append("user_id")
        if not shipping_address:
            missing_after_enrich.append("shipping_address")

        if missing_after_enrich:
            invalid_rows.append(
                (
                    raw_id,
                    event_id,
                    order_id,
                    missing_after_enrich,
                    ingested_at,
                    "Missing required fields after enrichment attempt",
                )
            )
            continue

        orders_rows.append(
            (
                order_id,
                user_id,
                product_id,
                product_name,
                shipping_address,
                current_stage,
                current_status,
                event_type,
                occurred_at,
                reason_code,
                hold_ops_status,
                hold_ops_note,
                hold_ops_operator,
                hold_ops_updated_at,
                raw_id,  # âœ… raw_reference_id ë°˜ë“œì‹œ í¬í•¨
            )
        )

    return events_rows, orders_rows, invalid_rows


# =============================================================================
# Kafka offset commit (ì •í™•íˆ)
# =============================================================================
def commit_offsets_exactly(consumer, last_offsets: Dict[Tuple[str, int], int]):
    """
    âœ… ë°°ì¹˜ì—ì„œ ì²˜ë¦¬í•œ ë§ˆì§€ë§‰ offset ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ì»¤ë°‹
    - Kafka commitì€ "ë‹¤ìŒì— ì½ì„ offset"ì„ ë„£ì–´ì•¼ í•˜ë¯€ë¡œ last_offset+1
    """
    if not last_offsets:
        return

    offsets = {}
    for (topic, partition), last_offset in last_offsets.items():
        tp = TopicPartition(topic, partition)
        offsets[tp] = OffsetAndMetadata(last_offset + 1, None)

    consumer.commit(offsets=offsets)


# =============================================================================
# ë°°ì¹˜ ì²˜ë¦¬ (íŠ¸ëœì­ì…˜ 1ë²ˆ)
# =============================================================================
def process_batch(conn, consumer, messages):
    """
    - messages: Kafka ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    - íŠ¸ëœì­ì…˜ 1ë²ˆì— orders_raw/events/orders/orders_invalidë¥¼ ë²Œí¬ ì²˜ë¦¬
    - ì„±ê³µí•˜ë©´ ì •í™•í•œ offset commit
    """
    if not messages:
        return

    cur = conn.cursor()

    try:
        # -------------------------------------------------------------
        # 1) orders_raw: âœ… í•­ìƒ ì €ì¥ + raw_id ë°˜í™˜
        # -------------------------------------------------------------
        orders_raw_rows, last_offsets = build_orders_raw_rows(messages)

        # execute_valuesë¡œ bulk insert + RETURNING raw_id
        # - psycopg2ì—ì„œëŠ” RETURNINGì´ ìˆì„ ë•Œ cur.fetchall()ë¡œ ê²°ê³¼ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ
        execute_values(
            cur,
            SQL_INSERT_ORDERS_RAW_VALUES_RETURNING,
            orders_raw_rows,
            page_size=min(BATCH_SIZE, 1000),
        )
        returned = cur.fetchall()
        raw_ids = [row[0] for row in returned]  # [(raw_id,), ...] -> [raw_id, ...]

        # ì•ˆì „ì¥ì¹˜: ê¸¸ì´ ë§¤ì¹­ í™•ì¸
        if len(raw_ids) != len(messages):
            raise RuntimeError(
                f"orders_raw RETURNING raw_id count mismatch: raw_ids={len(raw_ids)} messages={len(messages)}"
            )

        # -------------------------------------------------------------
        # 2) events / orders / invalid rows êµ¬ì„±
        # -------------------------------------------------------------
        events_rows, orders_rows, invalid_rows = build_events_orders_invalid_rows(
            cur, messages, raw_ids
        )

        # -------------------------------------------------------------
        # 3) events: ê°€ëŠ¥í•œ í•œ í•­ìƒ insert
        # -------------------------------------------------------------
        if events_rows:
            execute_values(
                cur,
                SQL_INSERT_EVENTS_VALUES,
                events_rows,
                page_size=min(BATCH_SIZE, 1000),
            )

        # -------------------------------------------------------------
        # 4) orders_invalid: ì´ìƒ ë°ì´í„° ê¸°ë¡
        # -------------------------------------------------------------
        if invalid_rows:
            execute_values(
                cur,
                SQL_INSERT_ORDERS_INVALID_VALUES,
                invalid_rows,
                page_size=min(BATCH_SIZE, 1000),
            )

        # -------------------------------------------------------------
        # 5) orders: ì •ìƒ ë°ì´í„°ë§Œ upsert (raw_reference_id í¬í•¨)
        # -------------------------------------------------------------
        if orders_rows:
            execute_values(
                cur,
                SQL_UPSERT_ORDERS_VALUES,
                orders_rows,
                page_size=min(BATCH_SIZE, 1000),
            )

        # -------------------------------------------------------------
        # 6) DB commit ì„±ê³µ í›„ offset ì»¤ë°‹
        # -------------------------------------------------------------
        conn.commit()
        commit_offsets_exactly(consumer, last_offsets)

        print(
            f"âœ… [BATCH OK] size={len(messages)} "
            f"(raw={len(orders_raw_rows)}, events={len(events_rows)}, orders={len(orders_rows)}, invalid={len(invalid_rows)})"
        )

    except Exception as e:
        conn.rollback()
        print(f"âŒ [BATCH FAIL] size={len(messages)} error={e}")
        raise
    finally:
        try:
            cur.close()
        except Exception:
            pass


# =============================================================================
# ë©”ì¸ ë£¨í”„
# =============================================================================
def main():
    print("ğŸ“¨ Kafka Consumer ì‹œì‘")
    print("=" * 60)
    print(f"- topic      : {KAFKA_TOPIC}")
    print(f"- bootstrap  : {KAFKA_BOOTSTRAP_SERVERS}")
    print(f"- group_id   : {KAFKA_GROUP_ID}")
    print(f"- offset     : {AUTO_OFFSET_RESET}")
    print(f"- batch_size : {BATCH_SIZE}")
    print(f"- flush_sec  : {FLUSH_EVERY_SEC}")
    print("=" * 60)

    # âœ… enable_auto_commit=False : DB commit ì„±ê³µ í›„ì—ë§Œ offset ì»¤ë°‹í•˜ë ¤ê³ 
    consumer = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=[KAFKA_BOOTSTRAP_SERVERS],
        group_id=KAFKA_GROUP_ID,
        auto_offset_reset=AUTO_OFFSET_RESET,
        enable_auto_commit=False,
        value_deserializer=lambda x: json.loads(x.decode("utf-8")),
        max_poll_records=BATCH_SIZE,
    )

    conn = connect_db_with_retry()

    # âœ… ë°°ì¹˜ ë²„í¼
    buffer = []
    last_flush_ts = time.time()

    try:
        while True:
            # -------------------------------------------------------------
            # (1) ì´ë¯¸ bufferê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°(ì§ì „ ë°°ì¹˜ ì‹¤íŒ¨) -> ì¬ì‹œë„ ìš°ì„ 
            # -------------------------------------------------------------
            if buffer:
                time_due = (time.time() - last_flush_ts) >= FLUSH_EVERY_SEC
                size_due = len(buffer) >= BATCH_SIZE

                if time_due or size_due:
                    try:
                        process_batch(conn, consumer, buffer)
                        buffer.clear()
                        last_flush_ts = time.time()
                    except Exception:
                        # ê°™ì€ buffer ìœ ì§€í•œ ì±„ë¡œ ì ê¹ ì‰¬ê³  ì¬ì‹œë„
                        time.sleep(1.0)
                    continue

            # -------------------------------------------------------------
            # (2) Kafka pollë¡œ ë©”ì‹œì§€ ìˆ˜ì§‘
            # -------------------------------------------------------------
            records = consumer.poll(timeout_ms=200)  # 0.2ì´ˆ ê¸°ë‹¤ë ¸ë‹¤ê°€ ë¬¶ì–´ì„œ ê°€ì ¸ì˜¤ê¸°
            for _tp, msgs in records.items():
                buffer.extend(msgs)

            if not buffer:
                continue

            # -------------------------------------------------------------
            # (3) flush ì¡°ê±´
            # - 1000ê°œ ì´ìƒ or ì¼ì • ì‹œê°„ ê²½ê³¼
            # -------------------------------------------------------------
            time_due = (time.time() - last_flush_ts) >= FLUSH_EVERY_SEC
            size_due = len(buffer) >= BATCH_SIZE

            if not (time_due or size_due):
                continue

            # -------------------------------------------------------------
            # (4) ë°°ì¹˜ ì²˜ë¦¬
            # -------------------------------------------------------------
            try:
                process_batch(conn, consumer, buffer)
                buffer.clear()
                last_flush_ts = time.time()
            except Exception:
                # ì‹¤íŒ¨í•˜ë©´ buffer ìœ ì§€ â†’ ë‹¤ìŒ ë£¨í”„ì—ì„œ ì¬ì‹œë„
                time.sleep(1.0)

    except KeyboardInterrupt:
        print("\nğŸ›‘ Consumer ì¢…ë£Œ")
    finally:
        # ì¢…ë£Œ ì§ì „ì— ë‚¨ì€ buffer flush ì‹œë„ (ì„±ê³µ ì‹œ offset ì»¤ë°‹)
        if buffer:
            try:
                print(f"â„¹ï¸ ì¢…ë£Œ ì „ ë‚¨ì€ ë©”ì‹œì§€ flush ì‹œë„: {len(buffer)}ê±´")
                process_batch(conn, consumer, buffer)
            except Exception:
                print("âš ï¸ ì¢…ë£Œ ì „ flush ì‹¤íŒ¨ (offset ë¯¸ì»¤ë°‹ ìƒíƒœë¡œ ì¢…ë£Œ)")

        try:
            conn.close()
        except Exception:
            pass

        try:
            consumer.close()
        except Exception:
            pass

        print("âœ… DB / Consumer ì •ìƒ ì¢…ë£Œ")


if __name__ == "__main__":
    main()